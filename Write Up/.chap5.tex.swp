\chapter{Methods \& Procedures}

\section{System Overview}

Despite the fact that two rather different algorithms will be used the system is designed to operate more or less the same with the genetic algorithm or support vector machine components as loosely coupled modules to avoid having to redesign the system for both approaches.  Web requests will be processed through a parser that looks for various aspects related to each of the three possible web attacks.  The results are then output and can be used as input for either the genetic algorithm, support vector machine, or potentially another algorithm that could extend the testing.  Finally, the testing modules will output the results to a file that can be processed by graphing tools (Figure \ref{fig:sys}), in this case the R programming language will be used to create graphs that can be used to drawn conclusions on the two approaches. % cite R

\begin{figure}
	\label{fig:sys}
	\includegraphics[width=450px]{./assets/img/system.png}
	\caption{Overview of the system}
\end{figure}

\section{Gathering Test Data}

All data will be gathered from as close to a real-world scenario as possible.  In order to do so, automated enumeration exploiting tools will be used to gather a large sample size of varied attacks (Table \ref{tab:tools}).  
In order to gather SQL injection attacks the popular tool sqlmap will be used, for XSS attacks Grabber and XSSer will be used. %cite the tools
These tools will be let loose on a private apache web-server that is hosting a very simple database connected application. %cite apache
In regards to gathering a large amount of RFI attacks it will require generation of a large sample size, as the tooling for these types of attacks is rather limited and the tools that do exist do not perform large scale enumeration attacks and instead attempt to compromise the server as fast as possible. % cite fimap
Because RFI maps are the simpliest in terms of their design and variations, a simple automated Python script can be used to generate a large amount of attacks using a heavy amount of randomization.  Finally, it is nessecary to also include some normal web requests which are not attacks to test for false positives.  This can easily be done by using the application as well as other websites normally, submit form inputs, etc, and collecting all of the resulting HTTP requests.  This can be done with a browser extension for Mozilla Firefox, HTTPFox. % cite http fox

\begin{table}
	\centering
	\label{tab:tools}
	\begin{tabular}{|l|l|}
	\hline
		\textbf{Request Type} & \textbf{Generation Method}\\
	\hhline{|=|=|}
		SQL Injection & sqlmap\\
	\hline
		XSS Attack & grabber, xsser\\
	\hline
		RFI Attack & randomized generation\\
	\hline
		Normal Requests & httpfox\\
	\hline
	\end{tabular}
	\caption{Breakdown of test data generation}
\end{table}

Like most web-servers, Apache has the ability to log all of the requests that it serves to a file.  The data that we need to parse for the genetic algorithm or support vector machine is the GET or POST HTTP request line.  The final step of gathering the testing and training data is to compile the log files and strip out the unneeded information so it can be passed to the parser.  Another small Python script can be used to generate a test file using these large banks of the correct size and proportions, this script will generate a file where each line contains the request line content and what type the request is, either SQLi, XSS, RFI, or not an attack.

\section{Parsing the Requests}

With the completed test file(s) containing the proper amount of each attack and normal requests, the next step is to parse each request into numeric values so that the algorithms can work on them.  Each request has their own respective features that are worth identifying, with more features needing to be identified for the genetic algorithm than for the SVM.  These features have been identified for each request type by previous research (Table \ref{tab:features}) but is important to distinguish the meaning of each as well as what can and cannot be detected in this way. % cite main papaer

\begin{table}
	\centering
	\label{tab:features}
	\begin{tabular}{|p{1.5in}|p{4.5in}|}
	\hline
		\textbf{Request Type} & \textbf{Features}\\
	\hhline{|=|=|}
		SQL Injection & \# SQL keywords, is encoded, \# fields containing SQL keyword, attack variant\\
	\hline
		XSS Attack & \# of HTML or script keywords, is encoded, \# fields containing a HTML or script keyword, attack variant\\
	\hline
		RFI Attack & \# of URLs, is encoded, \# of commands, attack variant\\
	\hline
	\end{tabular}
	\caption{Parseable Features}
\end{table}

The number of SQL keywords or reserved words is obtained by using a comprehensive list provided by the Oracle and MySQL DBMS documentation. % cite Oracle and MySQL documentation
This works well with our test environment as well, as the DBMS our application is using is MySQL so many of the automated attacks will use MySQL specific vulnerabilities.  Similarly, the HTML and javascript keywords were provided by the official W3C documentation and the official PHP related commands were sourced from PHP's main documentation.  % cite W3schools and php docs

Requests are capable of being encoded as well to further evade detection (Section %site chapter).
and so this is something that can be easily detected and recorded.  HTTP requests, usually GET requests specifically, will contain along with them fields and the information that they carry to the application code.  This information can be user supplied information (ex. a username or password) or application supplied information (ex. the current page number), either way it is able to be directly modified by a user and is where injections and malicious code is likely to lie (Figure \ref{fig:sampleRequest}).  For this reason, it is good to make a distinction between just a total number of keywords found and the number of fields that actually contain keywords to get a more complete picture.  Lastly, all of the discussed attack variants (Section % cite the chapters with it).
can be detected with the exception of \emph{Stored Procedure SQL injections}, which brings up the limitations of this type of parsing (Section % cite chap 7. 

\begin{figure}
	\centering
	\label{fig:sampleRequest}
	https://duckduckgo.com/?\hl{q=HTTP+Request}\&\hl{t=vivaldi}\&\hl{ia=web}
	\caption{A sample HTTP request with fields highlighted}
\end{figure}

The parser makes heavy use of regular expressions (Appendix A % cite)
to determine much of this information such as the keywords or contents of the fields and is designed to overcome common evasion tactics.  One such tactic is padding the alternate encodings of the request, instead of using the common two byte hexadecimal conversion of the ASCII values, several zeros can be appended to the two bytes to confuse simple parsers using built-in decoding libraries.  Another issue that had to be overcame was not double-counting keywords that were a prefix to another keyword, which is done simply by associating each word with its prefix combinations.  This results in only a minor amount of extra computation as there is not many keywords with prefixes.

\begin{algorithm}[H]
	\setstretch{1.0}
	
	\KwData{File with HTTP Requests and their true type}
	\KwResult{Resulting test file with every request stored along with the parsed features for the three types of web threats in the follwing order Original > SQLi > XSS> RFI}
	
	read in input file\;
	\For{line in input file}{
	
		\If{for SVM testing}{
			disregard encoding and attack variant features\;
		}
		pass request to each parsing module (sql, xss, and rfi)\;
		store original request and type in resulting file\;
		\For{each parsed result}{
			\eIf{for genetic algorithm}{
				
				convert result to binary based on the maximum lengths of each segment\;
				\If{decimal value exceeds maximum value in segment}{
					use maximum allowed value in segment\;
				}
				store complete bitstring in file on new line\;
			}{
			store decimal values into file on new line\;
			}
		}
	}	
	\caption{General overview of parsing procedure}
\end{algorithm}

Full documentation on the usage of the parser (Appendix). % forward reference

\section{Genetic Algorithm Based Signature Detection}

\subsection{Measured Metrics}

\subsection{Algorithm and Testing Procedure}

\section{Support Vector Machine Detection}


\subsection{Metrics}

\subsection{Algorithm and Testing Procedure}


