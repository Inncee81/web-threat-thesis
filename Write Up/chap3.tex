\chapter{Current Detection and Prevention Methods} \label{sec:sectionThree}

\section{Prevention through Development} \label{sec:preventDevel}

Web attacks can be significantly damaging to an organization in many ways and so detection and prevention is of very high importance.  These attacks and subsequent problems are not only limited to only small time organizations as well, with many high profile websites are becoming victims to large but similar attacks as well.  Some studies state over 90\% of web applications are vulnerable to SQL injections alone \cite{detectionAndPreventionSQL}.  Additionally, many web applications are vulnerable due to their usage of end-of-life software.  PHP is the most dominant programming language used for websites accounting for over 80\% of the market share, however of the websites that use PHP, 95\% of them are still on version 5 \cite{phpStats}.  As of the beginning of 2017 PHP 5 is no longer actively supported and it is recommended to switch to version 7 which is being actively supported, updating will bring with it all of the latest security fixes that could make a website easily exploitable otherwise \cite{phpVersions}\cite{wordpress}.

The best way to stop these web attacks is to prevent the vulnerabilities from existing in the first place, this is accomplished on the development side.  Despite the fact that the majority of the attacks are well documented and understood, as safeguards are preventions are put in place to protect one aspect of an application, attackers shift their efforts to look for the next weakest area.  From a security standpoint you need to assume your application is not bulletproof and you have only mitigated the risk but never removed it completely.  As a result of this rationale, prevention alone is not enough as there is always the possibility that an attack finds a way past the safeguards and so detection must work hand in hand with prevention.

The simpliest and most common method to prevent these attacks targeting the application layer of the web application is to never allow user input to be directly concatenated into any command that interacts with the server-side.  This is done by making use of what is called prepared statements, you instead construct the entire query or command and then pass in your data from the user as parameters.  This allows the server to distinguish between data and code no matter what kind of input is supplied by the user.  Likewise, whenever data is accessed from storage to be displayed to the user, it should not be directly inserted into a command that could potentially treat it as arbitrary code.  This will prevent issues such as stored XSS attacks or RFIs from occuring where stored code is inserted into the flow of the application code  \cite{owaspSQLPrevention}.  Many languages have different ways of accomplishing this but the most simpliest way is to strip out the portions of text that cause it to be interpreted as code, but a more comprehensive way is to filter the HTML against some form of filter or whitelist \cite{htmlPurifier}.

These are other measures that can be taken to mitigate the attacks but they mostly pertain to the environment on which the application is ran on and specifically for SQL injection prevention rather than all attack types.  Seperate database users should be made for each application and they should have the least amount of privledge possible.  In the event something is compromised, that database is at least isolated and other applications are uneffected.  A second measure that can be taken is to use views extensively instead of using direct queries for all database interactions as it allows access to the tables to be denied and instead only to the specially tailored views.  These strategies embrace the least privledge concept, where it is an unnessecary risk to be privy to more information or have more access than you truly need \cite{owaspSQLPrevention}.

Research has been done on examining the code of potentially XSS vulnerable applications to determine their vulnerability and was able to accurately detect the vulnerable code with no false positives or negatives.  So it is clear that modifying the code itself to be more secure should always be the top priority if it is possible to determine if a particular file is vulnerable that easily and accurately \cite{xssdm}.

\section{Signature Based Detection} \label{sec:sigDetection}

A traditional and common way of detecting security threats is the use of signatures, however many of these signature based tools are more suited for the lower layers of the OSI model rather than the higher layers such as the application and presentation layers.  These tools are referred to as Intrusion Detection Systems (IDS) and many rely on regular expressions and other pattern matching techniques with signatures produced using previous attacks methods, one example of such a tool is Snort \cite{mainPaper}.  Therefore, as long as there is an adequate number of signatures that cover the broad spectrum of possible attacks then the detection approach can be quite successful.  

However signature based detection systems as well as other IDS systems are not completely fail proof.  One of the biggest problems is the frequency of false positives, or in other words when the system believes something that is not harmful is harmful.  Of course the opposite is also true and IDS systems can let attacks slip by unnoticed, this can be a result of the attacks using various tricks to evade detection such as using alternate encodings or fragmenting packets or just simply being a new form of attack \cite{onTheVerification}.  In addition, if the IDS is signature based than what is a more likely explanation for these accuracy problems is a lack of signatures that are either more accurate or cover these new undocumented attacks.  It is becoming too impractical to produce these signatures fast enough due to the countless variants of the attacks and they are commonly designed to go unnoticed rather than spread as fast as possible like with conventional malware \cite{trendMicro}.  To give an idea on how difficult of a problem this is to solve with signatures alone, an average of 5,000 new software vulnerabilities have been identified per year; along with the number of unique malware programs alledgely in the tens of millions and doubling every year.  With these rapidly rising trends, it is clear that the malicious user is easily always ahead of the detection tools.  Static solutions including signature sets are becoming less and less practical every year for these reasons, and the current short comings of the detection systems to keep up themselves proves that point further \cite{onTheVerification}.

However, this problem is not unique to the web threat world, although signature based detection is much more suited for the traditional desktop computer application virus scanning practices have had to adapt as well to a similar problem.  Virus scanning is probably the best example of signature based detection in action, where malware is collected, a signature is developed to detect it, and then sent out to the masses as fast as possible.  However, some types of viruses have begun to exploit this by transforming their own code when transferring which would require an entire new signature to detect.  These so called metamorphic viruses are not impossible to defeat but they require approaching the idea of scanning for a virus completely differently than just collecting a single signature.  Such techniques include but are not limited to hidden Markov models or reversing the morphing process of the malware itself \cite{metaInvincible}\cite{metaHunting}.  If the area where signature based detection has proven to be the most strongest has had to adapt it's tactics to deal with the changing security climate, then by extension so to it does the web threat detection ecosystem.

\section{Modern Methods of Detection}

In order to overcome these challenges for detecting web threats, some people have suggested a multiple layered approach will provide the best defense.  Such a system would not only have multiple layers of detection involving signatures and other techniques but also feedback loops to update the protection systems for improved future detection.  A multi-layered approach would also be able to address all levels of the network rather than a single system that can only handle a subset of the layers such as the network or application layers.  This approach would also enable portions of the processing to be centralized and in the cloud while other areas are closer to the endpoints.  Traditional techniques like signature detection would still be used, but it would be able to be augmented with additional techniques such as behaviour analysis as often times web attacks are carried out in massive enumeration attempts and not just a single bad request.  One final point is such a solution would allow for global collaboration to contribute to reputation lists, whitelists, and the like to further solve the problem of a growing threat instead of having the same tools deployed in multiple areas and poor solutions for consistent updates \cite{trendMicro}.

A multi-layered approach combines the best of the old traditional techniques with new potentially better solutions, and even more interestingly suggests a system that can inherently learn and improve as a core trait; very similar to the technique in question in this research.








