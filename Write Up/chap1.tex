\chapter{Introduction}
\section{Problem Definition}\label{sec:probDefinition}

As the Internet grows in usage both in the number and types of devices it serves the attack surface grows along with it.  There are many attacks that are considered common and should always be a top priority when developing a new Internet connected application (Section \ref{sec:sectionTwo}).  However, prevention is not the only task when dealing with these threats as detection is equally as important.  Among many reasons, one scenario is if an attack known as a zero-day attack which has no established prevention method was just discovered and should be considered to already be causing damage.  In this scenario, detection is the first line of defense to inform those who are in a position to take nessecary actions to stop it.  

Conventional means of detection typically involved manually creating detection signatures from known attacks in order to detect ones of a similar nature.  However this creates multiple problems: the process of creating these signatures is slow and requires a reference attack, the created detection signatures will only detect attacks that are very similar to the reference, and finally it assumes the original attack will not change from its original state and can be redetected using the same signature (Section \ref{sec:sectionThree}).  Therefore as of recently the focus has shifted to using other techniques to attempt to improve on the short comings of the conventional approaches by using machine learning techniques, such as genetic algorithms (Section \ref{sec:sectionFour}).  These approaches show great promise, but no system is perfect, and with the research being fairly new the results and technqiues have not been as critically analyzed as much as the more established methods have been.

\section{Objective}\label{sec:objectives}

The objective of this research is first and foremost to more critically analyze the genetic algorithm approach presented in previous research, in addition to comparing it's performance to a new approach using a support-vector machine to classify the requests as threats or not threats instead.  The critical analysis of both machine learning techniques will involve determining the success rate of detection and how often attacks are misidentified either as a false positive or the wrong attack type.  Through this information a more complete picture of the performance of the algorithms can be developed and much more informed conclusions can be drawn from it.  Another objective of the research is to ensure testing is done accurately and fairly between the two algorithms, this means testing and training with the same data across both techniques, and the test data is different from the training data.  Lastly, in order to correct the requests which are represented with ordinary text into usable data, a simple parser must be made that is tailored to each attack type's nuances.

\subsection{Expected Results}\label{sec:expResults}

It is expected that the support-vector machine approach will outperform the genetic algorithm.  This is due to the fact that the support-vector machine is a classification tool and appears to be much more suited for the problem as a result.  In addition, the genetic algorithm relys on the mostly-random occurance of generating a new signature in order to detect additional attacks which suggests it may have highly-variable levels of success.  In contrast, the support-vector machine can get quite performance intensive when using more complicated kernel types and is more reliant on the training data.  If using more complex kernels is required to achieve good detection than the applicability of the approach may not be there.

\section{Thesis Overview}
\subsection{Scope and Limitations}\label{sec:scope}
This research is limited to only using genetic algorithms and support-vector machines for the application of detecting web threats and other machine learning techniques will not be examined.  To that effect, it is also limited to examining the following three application layer web threats: SQL injections, cross-site scripting and remote file inclusion (Section \ref{sec:sqliExplanation}, \ref{sec:xssExplanation}, \ref{sec:rfiExplanation}).  

The two techniques will be compared based on their detection results (success rate, false positives and incorrect detections), while time complexity and speed may be mentioned it will not be explicitly measured or recorded.

All tests will be carried out in a virtual environment using labelled data as to facilitate collection of results, this data will be collected using real automated exploit tools wherever possible.


