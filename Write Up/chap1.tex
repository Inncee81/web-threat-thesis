\chapter{Introduction}
\section{Problem Definition}\label{sec:probDefinition}

As the Internet grows in usage both in the number and types of devices it serves the attack surface grows along with it.  There are many attacks that are considered common and should always be a top priority when developing a new Internet connected application (Section \ref{sec:sectionTwo}) \cite{trendMicro, aSurveyOnWeb}.  However, prevention is not the only task when dealing with these threats as detection is equally as important.  Among many reasons, one scenario is the discovery of an attack known as a zero-day attack which has no established prevention method, such an attack has potentially already caused significant damage.  In this scenario, detection is the first line of defense to inform those who are in a position to take necessary actions to stop it.

Conventional means of detection typically involved manually creating detection signatures from known attacks in order to detect ones of a similar nature.  However this creates multiple problems: the process of creating these signatures is slow and requires a reference attack, the created detection signatures will only detect attacks that are very similar to the reference, and finally it assumes the original attack will not change from its original state and can be redetected using the same signature (Section \ref{sec:sectionThree}) \cite{onTheVerification, metaHunting, metaInvincible}.  Therefore as of recently the focus has shifted to using other techniques to attempt to improve on the short comings of the conventional approaches by using machine learning techniques such as genetic algorithms and support vector machines (Section \ref{sec:sectionFour}) \cite{testingNetworkBased, mainPaper, intrusionDetectionCostBased, intrusionDetectionNeural}.  These approaches show great promise, but no system is perfect.  Critical analysis is lacking due to the research being fairly new when compared to the more established, existing methods.

\section{Objective}\label{sec:objectives}

The objective of this research is first and foremost to more critically analyze the genetic algorithm approach presented in previous research, in addition to comparing its performance to a new approach using a support-vector machine to classify the requests as threats or not threats instead.  The critical analysis of both machine learning techniques will involve determining the success rate of detection and how frequent the misidentification of attacks either as a false positive or the wrong attack type is.  It can be determined through the use of this information, a much more informed conclusion and complete picture of the performance of the machine learning algorithms.  Ensuring the execution of the testing is accurate and fair between the two algorithms is another objective of the research, this means testing and training with the same data across both techniques and the test data is a different set than the training data.  Lastly, it is necessary to produce a simple parser tailored to each attack type's nuances in order to translate the requests from ordinary text into a form of usable data.

\section{Thesis Overview}
\subsection{Scope and Limitations}\label{sec:scope}
This research is limited to examining only the genetic algorithm’s and support-vector machine’s use for the application of detecting web threats, it does not extend to other additional machine learning techniques.  To that effect, it is also limited to examining the following three application layer web threats: SQL injections, cross-site scripting and remote file inclusion (Section \ref{sec:sqliExplanation}, \ref{sec:xssExplanation}, \ref{sec:rfiExplanation}).

The basis of the comparison of the two techniques will be their detection results (success rate, false positives and incorrect detections), while time complexity and speed is not a focus of the research and will not be explicitly measured or recorded.  The reasoning for this is the goal of the research is to focus on the actual viability of these approaches and how they would perform in a realistic setting rather than developing a very fast and efficient algorithm that has abysmal results in a practical setting.  To facilitate the collection of results, a virtual environment will execute all tests using labelled data, this data is collected using real automated exploit tools whenever possible.
