\chapter{Introduction}
\section{Problem Definition}

As the Internet grows in usage both in the amount of types of devices it serves, the attack surface grows along with it.  There are many attacks that are considered common and should always be a consideration when developing a new Internet connected application.  

However, prevention is not the only task for dealing with these attacks as detection is equally as important.  Among many reasons, one example is an attack is a zero-day attack and had no prevention method planned for and is already causing damage.  In this scenario detection is the first line to defense to inform those in the know to take nessecary actions.  

Conventional means of detection often involved manually created detection signatures from known attacks in order to detect ones of similar nature. However this creates multiple problems: the process of creating these signatures is slow and requires a reference attack, the detection signatures will only detect attacks that are very similar in the future, and finally it assumes that the original attack will not change from its original state and can be redetected using the same signature.

Therefore as of recently the focus has shifted to using other techniques to attempt to improve on the downsides of the conventional approaches using machine learning techniques, such as genetic algorithms. These approaches show great promise, but no system is perfect and the research is fairly new and they have no been as picked apart as more established methods have. \cite{test}

\section{Objective}

The objective of this research is to more critically analyze the genetic algorithm approach presented in previous research, in addition to comparing it to a new approach of using a support-vector machine to classify the requests as threats or not threats.

The critical analysis of both evolutionary algorithms will involve determining the success rate of detection and how often attacks are mis-identified either as a false positive or the wrong attack type.  Through this information a more complete picture of the algorithms can be gathered and much more informed conclusions can be drawn.

Another objective of the thesis is to ensure that testing is done accurately and fairly between the two algorithms, this means testing with the same data that is different from the data used in the training process.

Lastly, in order to turn the requests which are ordinary text-strings into usable data a simple parser must be made that is tailored to each attack types nuances.

\subsection{Hypothesis}

It is expected that the support-vector machine approach will outperform the genetic algorithm.  This is due to the fact that the support-vector machine is a classification tool and appears much more suited for the problem.  In addition, the genetic algorithm relys on the semi-random occurance of generating a new signature in order to detect further attacks which suggests it may have highly-variable levels of success.

However, the support-vector machine can get quite performance intensive when using more complicated kernel types and is reliant on the training data.  If using more complex kernels is required to achieve good performance, than the applicability of the approach may not be there.




\section{Thesis Overview}
\subsection{Scope and Limitations}
This research is limited to only using genetic algorithms and support-vector machines for this application and other evolutionary or machine learning techniques will not be examined.  To that effect, it is also limited to examining the three application layer web threats: SQL injections, cross-site scripting and remote file inclusion.  

The two approaches will be compared based on their detection results (success rate, false positives and incorrect detections), while time complexity and speed may be mentioned it will not be explicitly measured or recorded.

All tests will be carried out in a virtual environment that will have labelled data that would not be typical of the real world but is nessecary in order to determine results.


