\chapter{Methods \& Procedures}

\section{System Overview}

Despite the fact that two rather different algorithms will be used the system is designed to operate more or less the same with the genetic algorithm or support vector machine components as loosely coupled modules to avoid having to redesign the system for both approaches.  Web requests will be processed through a parser that looks for various aspects related to each of the three possible web attacks.  The results are then output and can be used as input for either the genetic algorithm, support vector machine, or potentially another algorithm that could extend the testing.  Finally, the testing modules will output the results to a file that can be processed by graphing tools (Figure \ref{fig:sys}), in this case the R programming language will be used to create graphs that can be used to drawn conclusions on the two approaches. % cite R

\begin{figure}
	\label{fig:sys}
	\includegraphics[width=450px]{./assets/img/system.png}
	\caption{Overview of the system}
\end{figure}

\section{Gathering Test Data}

All data will be gathered from as close to a real-world scenario as possible.  In order to do so, automated enumeration exploiting tools will be used to gather a large sample size of varied attacks (Table \ref{tab:tools}).  
In order to gather SQL injection attacks the popular tool sqlmap will be used, for XSS attacks Grabber and XSSer will be used. %cite the tools
These tools will be let loose on a private apache web-server that is hosting a very simple database connected application. %cite apache
In regards to gathering a large amount of RFI attacks it will require generation of a large sample size, as the tooling for these types of attacks is rather limited and the tools that do exist do not perform large scale enumeration attacks and instead attempt to compromise the server as fast as possible. % cite fimap
Because RFI maps are the simpliest in terms of their design and variations, a simple automated Python script can be used to generate a large amount of attacks using a heavy amount of randomization.  Finally, it is nessecary to also include some normal web requests which are not attacks to test for false positives.  This can easily be done by using the application as well as other websites normally, submit form inputs, etc, and collecting all of the resulting HTTP requests.  This can be done with a browser extension for Mozilla Firefox, HTTPFox. % cite http fox

\begin{table}
	\centering
	\label{tab:tools}
	\begin{tabular}{|l|l|}
	\hline
		\textbf{Request Type} & \textbf{Generation Method}\\
	\hhline{|=|=|}
		SQL Injection & sqlmap\\
	\hline
		XSS Attack & grabber, xsser\\
	\hline
		RFI Attack & randomized generation\\
	\hline
		Normal Requests & httpfox\\
	\hline
	\end{tabular}
	\caption{Breakdown of test data generation}
\end{table}

Like most web-servers, Apache has the ability to log all of the requests that it serves to a file.  The data that we need to parse for the genetic algorithm or support vector machine is the GET or POST HTTP request line.  The final step of gathering the testing and training data is to compile the log files and strip out the unneeded information so it can be passed to the parser.  Another small Python script can be used to generate a test file using these large banks of the correct size and proportions, this script will generate a file where each line contains the request line content and what type the request is, either SQLi, XSS, RFI, or not an attack.

\section{Parsing the Requests}

With the completed test file(s) containing the proper amount of each attack and normal requests, the next step is to parse each request into numeric values so that the algorithms can work on them.  Each request has their own respective features that are worth identifying, with more features needing to be identified for the genetic algorithm than for the SVM.  These features have been identified for each request type by previous research (Table \ref{tab:features}) but is important to distinguish the meaning of each as well as what can and cannot be detected in this way. % cite main papaer

\begin{table}
	\centering
	\label{tab:features}
	\begin{tabular}{|p{1.5in}|p{4.5in}|}
	\hline
		\textbf{Request Type} & \textbf{Features}\\
	\hhline{|=|=|}
		SQL Injection & \# SQL keywords, is encoded, \# fields containing SQL keyword, attack variant\\
	\hline
		XSS Attack & \# of HTML or javascript keywords, is encoded, \# fields containing a HTML or javascript keyword, attack variant\\
	\hline
		RFI Attack & \# of URLs, is encoded, \# of commands, attack variant\\
	\hline
	\end{tabular}
	\caption{Parseable Features}
\end{table}

The number of SQL keywords or reserved words is obtained by using a comprehensive list provided by the Oracle and MySQL DBMS documentation. % cite Oracle and MySQL documentation
This works well with our test environment as well, as the DBMS our application is using is MySQL so many of the automated attacks will use MySQL specific vulnerabilities.  Similarly, the HTML and javascript keywords were provided by the official W3C documentation and the official PHP related commands were sourced from PHP's main documentation.  % cite W3schools and php docs

Requests are capable of being encoded as well to further evade detection (Section %site chapter).
and so this is something that can be easily detected and recorded.  HTTP requests, usually GET requests specifically, will contain along with them fields and the information that they carry to the application code.  This information can be user supplied information (ex. a username or password) or application supplied information (ex. the current page number), either way it is able to be directly modified by a user and is where injections and malicious code is likely to lie (Figure \ref{fig:sampleRequest}).  For this reason, it is good to make a distinction between just a total number of keywords found and the number of fields that actually contain keywords to get a more complete picture.  Lastly, all of the discussed attack variants (Section % cite the chapters with it).
can be detected with the exception of \emph{Stored Procedure SQL injections}, which brings up the limitations of this type of parsing (Section % cite chap 7. 

\begin{figure}
	\centering
	\label{fig:sampleRequest}
	https://duckduckgo.com/?\hl{q=HTTP+Request}\&\hl{t=vivaldi}\&\hl{ia=web}
	\caption{A sample HTTP request with fields highlighted}
\end{figure}

The parser makes heavy use of regular expressions (Appendix A % cite)
to determine much of this information such as the keywords or contents of the fields and is designed to overcome common evasion tactics.  One such tactic is padding the alternate encodings of the request, instead of using the common two byte hexadecimal conversion of the ASCII values, several zeros can be appended to the two bytes to confuse simple parsers using built-in decoding libraries.  Another issue that had to be overcame was not double-counting keywords that were a prefix to another keyword, which is done simply by associating each word with its prefix combinations.  This results in only a minor amount of extra computation as there is not many keywords with prefixes.

\begin{algorithm}[H]
	\setstretch{1.0}
	\label{alg:parsing}
	\caption{General overview of parsing procedure}
	
	\KwData{File with HTTP Requests and their true type}
	\KwResult{Resulting test file with every request stored along with the parsed features for the three types of web threats in the follwing order Original > SQLi > XSS> RFI}
	
	read in input file\;
	\For{line in input file}{
	
		\If{for SVM testing}{
			disregard encoding and attack variant features\;
		}
		pass request to each parsing module (sql, xss, and rfi)\;
		store original request and type in resulting file\;
		\For{each parsed result}{
			\eIf{for genetic algorithm}{
				
				\eIf{lengths of segment 1 and 3 should be permuted for length testing}{
					\For{each segment length combination up to specified maximum}{
						convert result to binary based on the maximum lengths of each segment\;
						\If{decimal value exceeds maximum value in segment}{
							use maximum allowed value in segment\;
						}
						store result in a list\;
					}
					store complete list on a new line\;
				}{
				convert result to binary based on the maximum lengths of each segment\;
					\If{decimal value exceeds maximum value in segment}{
						use maximum allowed value in segment\;
					}
				store complete bitstring in file on new line\;
				}
			}{
			store decimal values into file on new line\;
			}
		}
	}	
\end{algorithm}

Full documentation on the usage of the parser (Appendix). % forward reference

\section{Genetic Algorithm Based Signature Detection}\label{sec:genIntro}

The method of the using a genetic algorithm for signature based detection is largely the same as the proposed and tested system in previous research with a few modifications. %cite main paper
One major difference is that instead of allowing the signatures to change to different attack type signatures (ex. SQLi to XSS) we specify what type of attack we want to search for and the algorithm uses that parsed result for every request.  This of course requires every original request to be parsed three times instead of just once, but it makes the most sense as if it is possible to transition between attack types so easily then there is no reason to differentiate between them in the first place.  This also causes later conclusions to not be influenced by factors that can not be measured.  If signatures were allowed to switch to different attacks types then it would be unknown if the results were due to the random nature of a genetic algorithm or the other variables being changed.  

The second major change is that the bitstring length for each signature is increased to avoid problems of exceeding the quantity in a segment.  In the previous research only 3 bits were used for the segments that count the number of segments or fields in the requests which only allows for a count up to 7.  In a real world setting the amounts for the number of fields and keywords can be much much larger and exceeding these values often creates a situation where there are many signatures that match which normally would not.  For example, if two signatures have 10 and 11 keywords respectively, with the old segment lengths they would be capped at 7, or 111, resulting in a match which should not have occured.  Therefore these segment lengths with a size problem have been extended to 6 bits allowing for counts up to 63 (Table \ref{tab:geneticSegments}).

\begin{table}
	\label{tab:geneticSegments}
	\begin{tabular}{|p{1.5in}|p{1.125in}|p{1.125in}|p{1.125in}|p{1.125in}|}
	\hline
	\multicolumn{1}{|p{1.5in}|}{\textbf{Request Type}} & \multicolumn{4}{p{4.5in}|}{\textbf{Segment Information}} \\ \hhline{|=|=|=|=|=|}
	\multirow{2}{*}{\textbf{SQL Injection}}            & \# of SQL Keywords & is encoded & \# of fields containing a SQL keyword & attack variant \\ \cline{2-5} 
		                                      & 6             & 1          & 6            & 3 \\ \hline  
	\multirow{2}{*}{\textbf{XSS Attack}}               & \# of HTML or javascript keywords & is encoded & \# of fields containing a HTML or javascript keyword & attack variant \\ \cline{2-5} 
		                                      & 6             & 1          & 6            & 3 \\ \hline
	\multirow{2}{*}{\textbf{RFI Attack}}               & \# of URLs & is encoded & \# of PHP commands & attack variant \\ \cline{2-5} 
		                                      & 6             & 1          & 6            & 3 \\ \hline  
	\end{tabular}
	\caption{Genetic algorithm default segment breakdown}
\end{table}

The genetic algorithm implemented is very standard, allowing for the following parameters to be changed:

\begin{itemize}
	\setstretch{1.0}
	\item Maximum population per iteration
	\item Maximum number of generations
	\item Number of iterations
	\item Mutation rate
	\item Elitist selection amount
\end{itemize}

Most of the genetic operators are fairly simple to implement with the most complex being selection.  The higher the bitstrings fitness is the more likely it should be selected.  There are several ways for this to be accomplished but for this implementation Roulette Wheel Selection is used, also referred to as Fitness Proportionate Selection (Algorithm \ref{alg:selection}).  This selection algorithm was choosen as the originally proposed genetic algorithm technique was fitness based %cite main paper
 and not reward based like other selection algorithms, as well as it is simple to implement and understand for these basic testing purposes.  For crossovering two individuals a single point crossover scheme is used.

\begin{algorithm}[H]
	\setstretch{1.0}
	\label{alg:selection}
	\caption{Basic pseudocode algorithm for Roulette Wheel Selection in \BigO{n}}
	\KwData{Fitness values of all individuals in population}
	\KwResult{The selected individual}
	
	$totalWeight\leftarrow0$\;
	\For{all individuals weights}{
		$totalWeight\leftarrow$totalWeight + weight\;
	}
	
	generate a random number between 0 and the totalWeight\;
	\For{all individuals weights}{
		
		subtract the weight from the random number\;
		\If{random number is less than 0}{
			return that individual
		}
	}
	
	\If{Unable to find an individual, error occured}{
		fall back condition is to return the last item\;
	}
\end{algorithm}

The genetic algorithm was written from scratch using the Python programming language and is designed to handle input files with single bitstrings from the parser or lines with several variations on the lengths of the bitstrings (Algorithm \ref{alg:genetic}).

\begin{algorithm}[H]
	\setstretch{1.0}
	\label{alg:genetic}
	\caption{Pseudocode algorithm for genetic algorithm}
	\KwData{Bitstrings for training and testing and all parameters for genetic algorithm}
	\KwResult{The optimized bitstrings that can be used for detection}
	
	\For{each bitstring of varying length}{
	
		\For{the number of generations}{
			
			remove duplicate bitstrings in the current population\;
			evaluate fitness for all individuals\;
			
			preserve the top elitist percentage into the new population called the offspring\;
			
			\While{offspring amount is less than maxmimum population allowed}{
				
				locate two individuals and perform a single point crossover \;
				add these two new individuals to the next population
			}
			
			trim the offspring to the maximum population just incase\;
			
			loop through every bit in every offspring with the chance to mutate it\;
			
			set the current population to the offspring
		}
		
		store the bitstring results for that length
	}
\end{algorithm}

\subsection{Testing Procedure}

In order to the test the genetic algorithm fairly, both training data and testing data will have equal proportions of all three attacks, 30\% for each attack and then 10\% of non attacks for false positive metrics (Table \ref{tab:trainingfile} \& \ref{tab:testfile}).  In addition, the testing data will be different requests than that used in training as this is the situation that these approaches would be exposed in the real world.  They would be trained using supervised learning and then used to identify unlabeled data entering the system so it does not make sense to test with the same data it is trained on.

\begin{table}
	\centering
	\label{tab:trainingfile}
	\begin{tabular}{|p{1.5in}|p{1.5in}|}
	\hline
		\textbf{Request Type} & \textbf{Number in Sample}\\
	\hhline{|=|=|}
		SQL Injection & 300 \\
	\hline
		XSS Attack & 300 \\
	\hline
		RFI Attack & 300 \\
	\hline
		Non Attacks & 100 \\
	\hhline{|=|=|}
		\textbf{Total} & 1000 \\
	\hline
	\end{tabular}
	\caption{Breakdown of the training file for genetic algorithms}
\end{table}	
	
\begin{table}
	\centering
	\label{tab:testfile}
	\begin{tabular}{|p{1.5in}|p{1.5in}|}
	\hline
		\textbf{Request Type} & \textbf{Number in Sample}\\
	\hhline{|=|=|}
		SQL Injection & 1500 \\
	\hline
		XSS Attack & 1500 \\
	\hline
		RFI Attack & 1500 \\
	\hline
		Non Attacks & 500 \\
	\hhline{|=|=|}
		\textbf{Total} & 5000 \\
	\hline
	\end{tabular}
	\caption{Breakdown of the testing file for both genetic algorithm and support vector machine}
\end{table}	

Every test will make use of the same training data and testing data and instead the parameters for the genetic algorithm will be altered to see if they make a difference on the results.  The genetic algorithm will be trained using the training data, which will output bitstrings that act as signatures that are optimized for detecting the particular attacks. Those signatures will be used to hopefully match correctly with the unseen testing data (Figure \ref{fig:gasys}).

\begin{figure}
	\label{fig:gasys}
	\includegraphics[width=450px]{./assets/img/gasys.png}
	\caption{Overview of the genetic algorithm system}
\end{figure}

In addition to the various parameters to the genetic algorithm having multiple iterations of the algorithm ran to generate a combined amount of signatures will be tested.  The thought is that the more signatures in your detection set the more likely you are to have one to detect the attacks so more iterations combined together should be able to detect more attacks.  This is also one of the main reasons why this technique was proposed, so that the genetic algorithm could generate additional signatures automatically for testing instead of relying on manually made patterns.  Also, testing the results with different lengths of bitstrings will also be attempted, the thought here goes back to the problem of overflowing a segment (Section \ref{sec:genIntro}).  Small segments should be able to detect more attacks as they should be able to more easily generate bitstrings that cover a wider range of attacks.  For example, a segment that can only hold a count of 1 would detect anything if it contained a keyword even if that request had a much greater amount, it would essentially become a flag.

\section{Support Vector Machine Detection}

The SVM detection will follow a similar process to the genetic algorithm but instead of changing the parameters of the algorithm, the training data will be changed instead.  This is because the parameters for the SVM that will make a difference are automatically optimized by a grid search approach provided by the same library used to implement the SVM in Python. %cite scikitlearn

SVM use various different kernel methods to determine a pattern in the given data, the kernels that will are being used from the provided library is a linear, polynomial degree 3, and RBF kernels.  This will be the only parameter that will be changed for the SVM but every test is ran on each kernel so it is consistent throughout the entire process.  The parameters for the SVM that are optimized by the gridsearch are gamma and the penalty cost, gamma only effects the polynomial and RBF kernels however.

The one main difference between the SVM and genetic algorithm approaches is that the SVM only requires two of the four segments and it does not need to be in binary form to support mutations (Table \ref{tab:svmSegments}).  These values are plotted on a simple X,Y plane which is then passed into the SVM to be trained, each of these values is labelled data cooresponding to either it is an attack or it is not (Algorithm \ref{alg:svm}).

\begin{table}
	\label{tab:svmSegments}
	\begin{tabular}{|p{1.5in}|p{2.25in}|p{2.25in}|}
	\hline
	\multicolumn{1}{|c|}{\textbf{Request Type}} & \multicolumn{2}{p{4.5in}|}{\textbf{Segments}}               \\ \hhline{|=|=|=|}
	\textbf{SQL Injection}                      & \# of SQL Keywords         & \# of fields containing a SQL keyword \\ \hline
	\textbf{XSS Attack}                      & \# of HTML or javascript keywords         & \# of fields containing a HTML or javascript keyword \\ \hline
	\textbf{RFI Attack}                      & \# of URLs         & \# of PHP commands \\ \hline
	\end{tabular}
	\caption{Genetic algorithm default segment breakdown}
\end{table}

\begin{algorithm}[H]
	\setstretch{1.0}
	\label{alg:svm}
	\caption{Pseudocode algorithm for support vector machine}
	\KwData{Segment information for each request}
	\KwResult{A trained SVM classifier that test data can be passed into and results gathered with}
	
	gather all segment information from training set\;
	
	pack into a numpy array\;
	
	\For{each kernel type ('linear', 'polynomial-3', 'rbf')}{
		
		\If{that kernel type has not already had its parameters optimized}{
			optimize using a GridSearch\;
			store the resulting parameters to save time on the next repeat use of the kernel\;
		}
		
		build the svm using scikit-learn and the optimized parameters\;
		train the classifier using the training vectors and targets\;
		
		pass all testing data through the classifier and record results\;
		
		plot the resulting graph for visual purposes\;
		store results of testing\;
	}
\end{algorithm}

\subsection{Testing Procedure}

For the SVM, there are several ways that the training data will be adjusted to produce different results (Figure \ref{fig:svmsys}).  The exact same testing set will be as what was used in the genetic algorithm, and for early tests the same training data will also be used but beyond the initial 1000 sample size, new training data must be used.  The first test will use the same proportion of 30\% for all three attack types and 10\% for non threats, this test will essentially be a fair comparison between the svm and the genetic algorithm approaches.  The second test will see if false positives can be reduced by increasing only the amount of false positives between the various tests.  And lastly, seeing if the number of incorrect attacks can be reduced, incorrect being identifying a request that is an attack but as the wrong type of attack, by increasing only the amount of the attacks we are \textbf{not} looking for.

\begin{figure}
	\label{fig:svmsys}
	\includegraphics[width=450px]{./assets/img/svmsys.png}
	\caption{Overview of the support vector machine system}
\end{figure}


