\chapter{Discussion} \label{sec:sectionSeven}

\section{Parser}
\subsection{Weaknesses}\label{sec:parseWeak}

The parser is the main point of failure for both of the approaches and therefore many of the parser's weaknesses become apparent when measuring the performance of the system.  If the output of the parser is not accurate than it is impossible to achieve consistently good performance either from the genetic algorithm or support vector machine. Although, the parsed results for both algorithms is the same, so the question becomes which algorithm can do a better job with the same input.

In the end, despite the fact that both approaches at least to avoid the use of regular expressions and pattern matching, they both depend on a parser to get the input needed for the respective algorithms and as a result there is always a chance the parser has been fooled or in accurate and influences the performance.  The question then becomes how good is the training process of these machine learning techniques in order to weed out these mistakes from the parser.  Part of the reason why the parser may be vulnerable to making these mistakes it only employs a simple token-based approach.  The parser cannot understand the logic or intent behind the requests by only picking up on keywords and fields within the requests.  If the parser was instead using some other kinds of data structures such as abstract syntax trees or program dependency graphs, then it may be possible to get a more complete picture of the request lead to greater accuracy.  This limitation is quite obvious as some attack types as well as even variants of the attack types covered such as \textit{stored SQL injections} cannot be detected as the information required is outside of the scope of the text request (Section \ref{sec:methodsParser}).

A final point of weakness is some attacks were much easier to parse than others.  SQL injections were definitely the most complicated of the three in addition to the keywords essentially being English words so there is always the potential of mistaking a normal word for an SQL word.  In contrast, the XSS and RFI keywords must follow a particular syntax so these attacks do not suffer as much from this problem.  The RFI attacks on the other hand were gathered using random generation in order to get a significant sample size for training and testing and so there are some inherent biases there.  All of these issues can skew find the results either a positive or negative manner.

\subsection{Strengths}

Despite some of the problems that could potentially be solved by moving away from a token-based approach, it is the fastest of the parsing techniques as building trees or dependency graphs adds a lot of extra computational time.  Additionally, for the purposes of detecting the selected features for each attack type a token-based approach is really all that is required to count up keywords and fields.  Using a more complicated technique than this might be simply overkill and reduce the overall speed of parsing when attempted to be of practical use in a real world environment.

Despite the weaknesses previous mentioned, parsing the results for these features is realistically the only way to determine the intent of the incoming application layer attack. There is no other useful information to pull out of the request text and no other deductions on the intent of the request are possible because it the request has not been processed as of yet.

\section{Genetic Algorithm}
\subsection{Parameter Testing Results}

The expected results in regards to changing the population size for the genetic algorithm were correct.  Allowing the algorithm to generate additional signatures per generation the detection rates increased and false positives and incorrect detections declined but did not always disappear completely.  This is the most obvious result as it is necessary to have a greater population in order to detect additional unique attacks, however the computational cost goes up along with it.  In addition, increasing population size alone will not result in perfect detection, as none of the attack types were able to achieve near 100\% successful detection and in the case of the RFI and XSS results, the detection rate actually went slightly down in the later tests (Section \ref{sec:resPopulation}).

Increasing the number of generations did not seem to have any discernable effect on the performance of the algorithm and in many cases it made the results worse and more variable.  Therefore the number of generations alone is not enough to optimize the results of the algorithm seen in an extreme example, the XSS detection, where it goes from a very high successful detection rate to 0\% with more generations (Section \ref{sec:resGeneration}).  Every generation the entire population is thrown out depending on the elitist pool, meaning that while the previous generation might have great signatures, the generated offspring could be terribly performing signatures as there is no correlation between the old and new signatures (Section \ref{sec:genDisadvantages}).

Mutation rate changes also had almost no discernable effect on the performance as well, in some tests the results improved while in the RFI detection for example the results flat line.  Mutation rate only slightly effects the genetic diversity and the of the performance is instead mostly determined by the generation of signature through crossovers, mutation rate alone is not significant enough to save the algorithms performance (Section \ref{sec:resMutation}).

Finally, the elitist pool changes had the best results out of all the tests in terms of its impact on the performance in a positive way.  For all three attack types, the more of the best performing individuals that were preserved the better, as most likely it allowed the transitions from generation to generation to be much more consistent and not be left up to pure chance (Section \ref{sec:resElitist}).

\subsection{Expanded Signature Set}

Doing multiple iterations of generating signature sets allowed for all three attack types to be able to hit near 100\% successful detection, which was not a result seen using any other tests so far.  With the exception of the RFI attack results most likely due to its inherent biases (Section \ref{sec:parseWeak}), XSS and SQL injection attacks increased their false positive and incorrect detections drastically unfortunately.  Having multiple iterations clearly causes this problem as the larger the signature set becomes the more likely it is to contain a signature that performs badly due to the random nature (Section \ref{sec:resIteration}).

\subsection{Influence of Segment Length}

Out of all of the test cases however, the segment length variations test is the most telling of the flaws of the genetic algorithm approach.  It is quite obvious from the results that smaller segment sizes result in higher success rates, false positives, and incorrect detections on average when compared to the larger segment sizes (Section \ref{sec:resSegment}).  This matches the expected result as not only does having a smaller segment size lead to inaccurate bitstrings that become artificial matches, but it also makes it much more likely for a poor performing bitstring to be generated and get through the fitness process (Section \ref{sec:genDisadvantages}).

\subsection{Strengths and Weaknesses} \label{sec:genDisadvantages}

The genetic algorithm approach does indeed seem to be able to achieve the claimed success rates and successfully generate new signatures can at times perform very well.  However, proof of the high variability and very poor of false positives and incorrect detections rates, can be found everywhere in the results.  It seems the approach works best when given existing well performing signatures, this is apparent through observing the results of the elitist pool testing.  Albeit in these tests it did not achieve the best success rates seen in other tests.  This is likely explained due to the use of fitness proportionate selection, which can cause the search to either stagnate or converge too quickly; by maintaining high performing bitstrings the chances of those bitstrings ever leaving the population decrease \cite{selectionPressure}.  Lastly, the genetic algorithm has a lower dependence on the accuracy of the parsed results compared to the SVM because the results are more of a result of the random generation of signatures rather than making improvements in the training process.

These results overall show the approach in quite a weak light and it seems ill-suited for this application.  The first of these flaws is how highly variable and random the approach is.  In order for an attack to be detected, a signature matching its parsed result must be generated, however the fitness algorithm does not rate how close the bitstrings are to this matching signature, rather it rates how well the signature detected multiple results and avoided mistakes.  It would be impractical to use a genetic algorithm to do this however, as it would be much easier to just construct signatures from parsed results of known attacks instead.  Therefore, the focus of the fitness algorithm seems to be on keeping the better performing bitstrings within the population until the end of the algorithm and hopefully kicking out the bad performing bitstrings; however, this does not always work out perfectly as was explained previously.  This is ultimately the root cause of the variability between generations and what causes odd anomalies in some of the results.  For these reason, the genetic algorithm seems to be a bloated random permutation generator that uses a fitness algorithm to remove the poorly performing bitstrings, and so this claim was tested (Section \ref{sec:randDiscussion}).

In addition, segment lengths of the bitstrings are very important.  If the bitstrings are too small there is a higher chance to generate a poorly performing signature as the search space is much too small.  To give an example, the proposed bitstring length was 10 bits which means 1024 unique combinations, meaning that since having duplicate bitstrings is redundant and they are removed, if the population size is around 1000 it is very likely that a bitstring representing a non-attack or incorrect attack type is generated and the fitness algorithm will not be remove it until the next generation.  Likewise, having such small segment lengths are not applicable to the real world, enumeration attacks can use dozens of keywords per segment and having such a small count in each segment results in artificial matches that would not occur otherwise.  For example, if every single SQL injection attack had a unique number of keywords but at least 7 per request, and the segment only had 3 bits to hold the number of keywords then the algorithm is only required to generate a single bitstring to detect \textbf{all} of them.  The signature is not truly matching with the attack, instead it is artificially padding the find results of the techniques.

\subsection{Comparison with Random Permutations}\label{sec:randDiscussion}

The results of using simple random permutation generator is quite telling of the poor performance of the genetic algorithm.  Not only are the results much more predictable and non-volatile, but by using 80\% of the well performing randomly permuted bitstrings the success rate was over 80\% on all three attack types with minimal false positives and incorrect detection, an unachievable result with the genetic algorithm.  In addition to these excellent results, this approach also completed in a fraction of the time as the bitstrings only had to be randomly generated once, and had fitness evaluated once rather than multiple times over again in the genetic algorithm along with additional selection, crossover, and mutation procedures.

This demonstrates how it really is just the random permutation process that is giving the genetic algorithm the results it has achieved, and therefore the genetic algorithm component is completely unnecessary.  While the amount of generated permutations exponentially grows with the size of the bitstring signature, the size used for the important segments that maintain the count of keywords and fields is double the size of the proposed length from previous research.

\section{Support Vector Machine}
\subsection{Comparison with Genetic Algorithm}

With the results from these tests it is quite clear which is the better approach out of the two, and that would be the SVM.  All three attack types achieved a successful detection rate between 90\% and 100\%, but also with very minimal false positive and incorrect detection rates, only showing up in the SQL injection results infact (Section \ref{sec:resComparison}).

\subsection{Reducing False Positives}

Increasing the number of false positives in the training set did indeed decrease the rate of false positive detection overall in the tests for SQL injections, as the other two attack types did not have substantial amounts of false positive detection to begin with.  This shows that training does indeed directly affect the performance of the SVM (Section \ref{sec:resNonThreat}).

\subsection{Reducing Incorrect Detections}

Similar to the results of the false positive training, the more incorrect results input into the training data, the better the SVM was able to avoid detecting them.  However as overall these results were not as much of an improvement, this may be due to the larger amount of training data having parsing mistakes that ended up effecting the results (Section \ref{sec:resIncorrect}).

\subsection{Strengths and Weaknesses} \label{sec:svmDisadvantages}

Overall, the SVM approach appears to be a much more suitable choice when compared to the genetic algorithm and even the random permutation trial approach.  One of the advantages is that the training is only required once for the SVM and then new attacks can be passed into it without the need for re-training or checking with thousands of signatures to see if any are a match.  With the proper kernel it also is able to run much faster, as for example, the RBF kernel produced good results but is quite complex and slow, the polynomial kernel as well is slow but produced very poor results in comparison showing that this is not the ideal kernel.  Therefore, the linear kernel which is also the fastest of the three and performed very well overall, seems to be the ideal and an overall improvement, otherwise the kernel complexity would be a major weakness of the SVM.  In addition, the SVM requires fewer features to be parsed from the request, only needing the keywords and the field counts, the attack variant and encoded flag are not required speeding up the parsing process as well.

The main weakness of the SVM approach is its strong dependence on the accuracy of the parsed results.  The algorithm cannot hope to for the training process to complete properly and produce accurate output if the results from the parser are not correct from the beginning.  But considering that both approaches were using the same training data and the SVM still came out majorly on top with results that are not even reproducible with the genetic algorithm; it does not seem to be too much of an issue for the detection performance.  Lastly, despite the mentioned biases with the RFI parsing, the fact that the SVM achieved no false positives or incorrect results for RFI detection is not a result of bias, as the XSS results went the same way and were all generated through real automated attack scripts.
