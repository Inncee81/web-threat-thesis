\chapter{Discussion} \label{sec:sectionSeven}

\section{Parser}
\subsection{Weaknesses}\label{sec:parseWeak}

The parser is the main point of failure for both of the approaches and therefore many of the parser's weaknesses become apparent when measuring the performance of the system.  If the output of the parser is not accurate than it is impossible to achieve consistently good performance either from the genetic algorithm or support vector machine. Although, both algorithms have been given the same parsed results so the question becomes which algorithm can do a better job with the same input.  

In the end, despite the fact that both approaches atleast to avoid the use of regular expressions and pattern matching, they both depend on a parser to get the input needed for the respective algorithms and as a result there is always a chance the parser has been fooled or in accurate and influences the performance.  The question then becomes how good is the training process of these machine learning techniques in order to weed out these mistakes from the parser.  Part of the reason why the parser may be vulnerable to making these mistakes it only employs a simple token-based approach.  The parser cannot understand the logic or intent behind the requests by only picking up on keywords and fields within the requests.  If the parser was instead made using other data structures such as abstract syntax trees or program dependency graphs then it may be possible to get a more complete picture of the request lead to greater accuracy.  This limitation is quite obvious as some attack types as well as even variants of the attack types covered such as \textit{stored SQL injections} cannot be detected as the information required is outside of the scope of the text request (Section \ref{sec:methodsParser}). 

A final point of weakness is some attacks were much easier to parse than others.  SQL injections were definitely the most complicated of the three in addition to the keywords essentially being english words so there is always the potential of mistaking a normal word for an SQL word.  In contrast, the XSS and RFI keywords must follow a particular syntax so these attacks do not suffer as much from this problem.  The RFI attacks on the other hand had to be randomly generated in order to get a significant sample size for training and testing and so there are some inherent biases there.  All of these issues can skew find the results either a positive or negative manner.

\subsection{Strengths}

Despite some of the problems that could potentially be solved by moving away from a token-based approach, it is the fastest of the parsing techniques as building trees or dependency graphs adds a lot of extra computational time.  Additionally, for the purposes of detecting the selected features for each attack type a token-based approach is really all that is required to count up keywords and fields.  Using a more complicated technique than this might be simply overkill and reduce the overall speed of parsing when attempted to be used in a real world environment.

Despite the weaknesses previous mentioned, parsing the results for these features is realistically the only way to determine the intent of the incoming application layer attack. There is no other useful information to pull out of the request text and no other deductions can be made on the intent of the request because it has not yet been executed.

\section{Genetic Algorithm}
\subsection{Parameter Testing Results}

In regards to changing the population size the expected results were observed.  As the algorithm was allowed to generate additional signatures per generation the detection rates increased and false positives and incorrect detections declined but did not always dissappear completely.  This is the most obvious result as a greater population is needed in order to detect additional unique attacks, however the computational cost goes up along with it.  In addition, increasing population size alone will not result in perfect detection, as none of the attack types were able to achieve near 100\% successful detection and in the case of the RFI and XSS results, the detection rate actually went slightly down in the later tests (Section \ref{sec:resPopulation}, Appendix \ref{app:resPopulation}).  

Increasing the number of generations did not seem to have any discernable effect on the performance of the algorithm and in many cases it made the results worse and more variable.  Therefore the number of generations alone is not enough to optimize the results of the algorithm seen in a extreme example, the XSS detection, where it goes from a very high successful detection rate to 0\% with more generations (Section \ref{sec:resGeneration}, Appendix \ref{app:resGeneration}).  Every generation the entire population is thrown out depending on the elitist pool, meaning that while the previous generation might have great signatures, the generated offspring could be terribly performing signaures as there is no correlation between the old and new signatures (Section \ref{sec:genDisadvantages}).

Mutation rate changes also had almost no discernable effect on the performance as well, in some tests the results improved while in the RFI detection for example the results flatline.  Mutation rate only slightly effects the genetic diversity and the of the performance is instead mostly determined by the generation of signature through crossovers, mutation rate alone is not significant enough to save the algorithms performance (Section \ref{sec:resMutation}, \ref{app:resMutation}).

Finally, the elitist pool changes had the best results out of all the tests in terms of it's impact on the performance in a positive way.  For all three attack types, the more of the best performing individuals that were perserved the better, as most likely it allowed the transitions from generation to generation to be much more consistent and not be left up to pure chance (Section \ref{sec:resElitist}, Appendix \ref{app:resElitist}).

\subsection{Expanded Signature Set}

Doing multiple iterations of generating signature sets allowed for all three attack types to be able to hit near 100\% successful detection, which was not a result seen using any other tests so far.  With the exception of the RFI attack results most likely due to it's inherent biases (Section \ref{sec:parseWeak}), XSS and SQL injection attacks increased their false positive and incorrect detections drastically unfortunately.  Having multiple iterations clearly causes this problem as the larger the signature set becomes the more likely it is to contain a signature that performs badly due to the random nature (Section \ref{sec:resIteration}, Appendix \ref{app:resIteration}).

\subsection{Influence of Segment Length}

Out of all of the test cases however, the segment length variations test is the most telling of the flaws of the genetic algorithm approach.  It is quite obvious from the results that smaller segment sizes result in higher success rates, false positives, and incorrect detections on average when compared to the larger segment sizes (Section \ref{sec:resSegment}, Appendix \ref{app:resSegment}).  This matches the expected result as not only does having a smaller segment size lead to inaccurate bitstrings that become artifical matches, but it also makes it much more likely for a poor performing bitstring to be generated and get through the fitness process (Section \ref{sec:genDisadvantages}).

\subsection{Strengths and Weaknesses} \label{sec:genDisadvantages}

The genetic algorithm approach does indeed seem to be able to achieve the claimed success rates and successfully generate new signatures can at times perform very well.  However, proof of the high variability and very poor of false positives and incorrect detections rates, can be found everywhere in the results.  It seems the approach works best when given existing well performing signatures, this can be seen through the results of the elitist pool testing.  Albeit in these tests it did not achieve the best success rates seen in other tests.  This is likely explained due to the use of fitness proportionate selection, which can cause the search to either stagnate or converge too quickly; by maintaining high performing bitstrings the chances of those bitstrings ever leaving the population decrease \cite{selectionPressure}.  Lastly, the genetic algorithm has a lower dependence on the accuracy of the parsed results compared to the SVM because the results are more of a result of the random generation of signatures rather than making improvements in the training process.

These results overall show the approach in quite a weak light and it seems ill-suited for this application.  The first of these flaws is how highly variable and random the approach is.  In order for an attack to be detected, a signature matching it's parsed result must be generated, however the fitness algorithm does not rate how close the bitstrings are to this matching signature, rather it rates how well the signature detected multiple results and avoided mistakes.  It would be impractical to use a genetic algorithm to do this however, as it would be much easier to just construct signatures from parsed results of known attacks instead.  Therefore,  the fitness algorithm's intent seems to instead be focused on keeping the better performing bitstrings in the population until the end of the algorithm and hopefully kicking out the bad performing bitstrings; however this does not always work out as was explained.  This is ultimately the root cause of the variability between generations and what causes odd anomalies in some of the results.  For these reason, the genetic algorithm seems to be a bloated random permutation generator that uses a fitness algorithm to remove the poorly performing bitstrings, and so this claim was tested (Section \ref{sec:randDiscussion}).

In addition, segment lengths of the bitstrings are very important.  If the bitstrings are too small there is a higher chance to generate a poorly performing signature as the search space is much too small.  To give an example, the proposed bitstring length was 10 bits which means 1024 unique combinations, meaning that since having duplicate bitstrings is redundant and they are removed, if the population size is around 1000 it is very likely that a bitstring representing a non-attack or incorrect attack type is generated and the fitness algorithm will not be remove it until the next generation.  Likewise, having such small segment lengths are not applicable to the real world, enumeration attacks can use dozens of keywords per segment and having such a small count in each segment results in artifical matches that would not occur otherwise.  For example, if every single SQL injection attack had a unique number of keywords but atleast 7 per request, and the segment only had 3 bits to hold the number of keywords then the algorithm is only required to generate a single bitstring to detect \textbf{all} of them.  The signature is not truly matching with the attack, instead it is artifically padding the find results of the techniques.

\subsection{Comparison with Random Permutations}\label{sec:randDiscussion}

The results of using simple random permutation generator is quite telling of the poor performance of the genetic algorithm.  Not only are the results much more predictable and non-volatile, but by using 80\% of the well performing randomly permutated bitstrings the success rate was over 80\% on all three attack types with minimal false positives and incorrect detection, a result that could not be achieved with the genetic algorithm.  In addition to these excellent results, this approach also completed in a fraction of the time as the bitstrings only had to be randomly generated once, and had fitness evaluated once rather than multiple times over again in the genetic algorithm along with additional selection, crossover, and mutation procedures.

This demonstrates how it really is just the random permutation process that is giving the genetic algorithm the results it has achieved, and therefore the genetic algorithm component is completely unnessecary.  While the amount of permutations that must be generated exponentially grows with the size of the bitstring signature, the size used for the important segments that maintain the count of keywords and fields is double the size of the proposed length from previous research.

\section{Support Vector Machine}
\subsection{Comparison with Genetic Algorithm}

With the results from these tests it is quite clear which is the better approach out of the two, and that would be the SVM.  All three attack types achieved a successful detection rate between 90\% and 100\%, but also with very minimal false positive and incorrect detection rates, only showing up in the SQL injection results infact (Section \ref{sec:resComparison}, Appendix \ref{app:resComparison}).  

\subsection{Reducing False Positives}

Increasing the number of false positives in the training set did indeed decrease the rate of false positive detection overall in the tests for SQL injections, as the other two attack types did not have substantial amounts of false positive detection to begin with.  This shows that training does indeed directly affect the performance of the SVM (Section \ref{sec:resNonThreat}, Appendix \ref{app:resNonThreat}). 

\subsection{Reducing Incorrect Detections}

Similar to the results of the false positive training, the more incorrect results input into the training data, the better the SVM was able to avoid detecting them.  However as overall these results were not as much of an improvement, this may be due to the larger amount of training data having parsing mistakes that ended up effecting the results (Section \ref{sec:resIncorrect}, Appendix \ref{app:resIncorrect}).

\subsection{Strengths and Weaknesses} \label{sec:svmDisadvantages}

Overall, the SVM approach appears to be a much more suitable choice when compared to the genetic algorithm and even the random permutation trial approach.  One of the advantages is that the SVM can be trained once and then new attacks can be passed into it without the need for re-training or checking with thousands of signatures to see if any are a match.  It also is able to run much faster if the proper kernel is choosen, as the RBF kernel produced good results but is quite complex and slow, the polynomial kernel as well is slow but produced very poor results in comparison showing that this is not the ideal kernel.  Therefore the linear kernel which is also the fastest of the three and performed very well overall, seems to be the ideal and an overall improvement, otherwise the kernel complexity would be a major weakness of the SVM.  In addition, the SVM requires fewer features to be parsed from the request, only needing the keywords and the field counts, the attack variant and encoded flag are not required speeding up the parsing process as well.

The main weakness of the SVM approach is it's strong dependence on the accuracy of the parsed results.  If the results from the parser are not correct then the algorithm can not hope to be trained properly and produce accurate output.  But considering that the same training data was used in both approaches and the SVM came out majorly on top and these same results can be reproduced, it does not seem to be too much of an issue for the performance.  Lastly, despite the mentioned biases with the RFI parsing, the fact that the SVM achieved no false positives or incorrect results for RFI detection is not a result of bias, as the XSS results went the same way and were all generated through real automated attack scripts.

