\chapter{Describing Machine Learning Approaches} \label{sec:sectionFour}

\section{Machine Learning} \label{sec:machineLearning}

Machine learning has become a popular topic in recent years as an answer to the sheer amount of data that has the potential to be processed.  Not only does the large amount of data require more advanced and intelligent ways to process it, but there is also a big incentives driving the desire to do so.  Through machine learning techniques and patterns additional meaning can be extracted from the data which can be used to draw conclusions or predictions \cite{dataManagementSystems}.  It is common to apply machine learning techniques to data mining tasks, examining the data and discerning additional information from it.  In the case of this research's problem area the mined data is the large amount of web requests.

This brings numerous benefits to a security application such as web threat detection, primarily it allows the system to have some sort of feedback mechanisms to improve its detection and perhaps even its prevention.  A system that is unable to learn overtime will only be able to adapt to new techniques designed by attackers to evade the current detection systems abilities with manual updates.  And as discussed before, manual updating is not only a very inefficient time consuming task but it is also just plain infeasible as the number and complexity of these attacks grows.  Identifying patterns from data is very useful considering that many attacks follow some kind of a basic syntax or format and while there are many ways to evade detection, typically there is still an identifiable common method or intent.

The way machine learning operates is by identifying a series of features in the data set in question, these features are the parameters used by the algorithm to learn and make better decisions.  The key distinction of machine learning is that the algorithms are not told what to do explicitly but instead is allowed to make its decision based on measurements such as performance \cite{supervisedMachineLearning}.

\subsection{Supervised Learning} \label{sec:supervisedLearning}

Machine learning algorithms can either make use of supervised learning or unsupervised learning.  In supervised learning the system begins with labelled data, data that states what the end result should be so the training of the system can be accurate without additional intervention.  There is also another type of learning called reinforcement learning where an external source informs the system how well it is working or not as it progresses.

Supervised learning has issues that must to be overcome however, the first of which being collecting the original data set.  If there is prior research or informed sources that can suggest what features to use, then the process is much more trivial, otherwise the features are often identifiable using a brute force method.  The problem with using a brute force method other than the computation time is the data has the potential to be noisy and miss important features which can lead to further problems.  Learning from extremely large datasets is very inefficient as well.  Therefore, minimizing the size of the data set while still maintaining the final performance of the system through a process called instance selection is generally preferred.  Lastly, having a large amount of features in the data set can also increase the complexity of the system.  To solve this remove irrelevant and redundant features whenever possible, but if many of the features depend on each other than removal may not be possible as this can lead to inaccuracies in the learning process.  To deal with these types of features construct new features or alter existing ones to be more concise and accurate, improving the entire system as a whole in the process.  One of the big steps in creating a machine learning system is selecting the right algorithm for the dataset, each which their own advantages, disadvantages and times where they are applicable (Section \ref{sec:sectionSeven}).

\section{Genetic Algorithm} \label{sec:genAlgorithm}

A genetic algorithm is a search-based algorithm that makes use of machine learning in order to locate optimal or near-optimal solutions for a particular problem.  This is what is meant by the term 'search' that is often used to describe this and other algorithms such as local search or simulated annealing, it is not about locating something in a particular data set but rather searching for the best possible answer.  Such algorithms, especially in the case of a genetic algorithm use fitness functions and possibly reward systems to distinguish between a better solution and one that should not be considered going forward \cite{searchBasedSoftwareEngineering}.

As the name suggests, a genetic algorithm mimics how genetic development in the real world works and how species evolve over time.  The algorithm begins with an initial population of individuals, an individual being a possible solution to the problem in question.  Every individual has it's fitness evaluated using some form of calculation tailored to suit the problem at hand; for our purposes for web threat detection the fitness will be evaluated with the following:

\begin{algorithm}[H]
	\setstretch{1.0}
	\centering

	$fitness = (\frac{correct\ detections}{\alpha})\ - (\frac{false\ positives}{\gamma})\ - (\frac{incorrect\ detections}{\beta \cdot 8})$ \\

	$\alpha \leftarrow The\ number\ of\ possible\ correct\ detections$ \\
	$\beta \leftarrow The\ number\ of\ possible\ incorrect\ detections$ \\
	$\gamma \leftarrow The\ number\ of\ possible\ false\ positives$ \\

	\caption{Fitness algorithm for use in genetic algorithm}
	\label{alg:fitness}
	\vspace{2cm}
\end{algorithm}

Correct detections with improve the fitness of a particular individual and the individual is more likely to be selected for genetic operators later on, whereas false positives and incorrect detections impact the fitness negatively, with incorrect detections having less of a negative impact than the former.  As an example, if we are looking for SQL injections, every deleted request that is a SQL injection is correct, if instead it is not an attack at all then it is a false positive and if it is actually a XSS or RFI attack then this is an incorrect detection.

In order for the genetic algorithm to produce a new population it makes use of genetic operators which commonly include performing crossovers and mutations to the individuals.  A selection algorithm chooses two individuals at a time in the population and then crossed-over to produce new offspring, there are many ways to perform a crossover but a single-point crossover will be the method used in this research.  In order to perform the crossover a position the first step is to select a position, referred to as a locus, for this research’s purposes this refers to a single segment.  The next step is to swap this segment with the other selected individual's respective segment to produce two new individuals with unique chromosomes, or in other words different configurations.  This process continues until the algorithm has produced enough new individuals to fill the next population, in addition, at the beginning of this process it is possible to mark some of the top individuals as elite and bring them into the new population unaltered.  Before continuing to the next generation every single allele, or piece of information in each individual has a small mutation chance, this is what causes the population to have some sort of diversity.  This entire process then repeats many times, each time being referred to as a generation and by the end of the process there should be a set of individuals that are closer to solving the problem \cite{matlabGenetic}.

\subsection{Current Genetic Algorithm Solutions} \label{sec:currentGenSolutions}

These genetic algorithm techniques are already applicable to web threat detection already, one particular paper focused on using variants of an attack to detect network related attacks.  While they may not have directly used a genetic algorithm in their solution, the core idea is very similar to how a genetic algorithm operates with generating different individuals and seeing if they fit a certain criterion.  These exploit variants are useful for testing signature based detection methods to see if it was possible to evade them and results showed it was.  This is proof that traditional models for detection cannot be made absolutely perfect and that using an approach similar to genetic algorithm techniques is worthwhile for at least evading detection \cite{testingNetworkBased}.

As of recently research has taken this idea and done the opposite, using a genetic algorithm directly to detect web attacks rather than to evade detection.  The genetic algorithm’s primary goal is to generate variants of attack detection signatures that can better detect SQL injections, XSS, and RFI attacks through the text-based web request logs.  The results of which were very promising with around a 90\% detection accuracy reported which exceeded the performance of a traditional regular expression signature based detection system \cite{mainPaper}.

These recent findings are the starting point for this research, improving the genetic algorithm approach and gathering more detailed results about its function and performance, as well as being the comparison point for another machine learning technique, support vector machines.

\section{Support Vector Machine} \label{sec:SVM}

A support vector machine's main technique for classifying data is to divide the data set into two or more categories with the largest margin possible between the separation(s), referred to as a hyperplane(s).  The reason for maximizing this buffer between categories is to reduce the chances of classification error as much as possible.  With the hyperplane(s) computed, points that lie within the margin are referred to as support vectors, hence the name, and it is these points which were used to calculate the hyperplane(s) in the first place with the other data points being ignored (Figure \ref{fig:svmmargin}).

\begin{figure}
	\includegraphics[width=450px]{./assets/img/svmmargin.png}
	\caption{Example of a linear seperated SVM \cite{supervisedMachineLearning}}
	\label{fig:svmmargin}
\end{figure}

The fact that the support vectors, which are usually a very small subset of the training vectors, determines the SVM is great because it means the speed does not significantly slow down with a larger amount of features.  However it is also realistic to imagine data where a division is not simple which can be overcome by using soft margins that allow for misclassifications or in more complex cases the data can be mapped to a higher dimensional space to open up other options of dividing the data.  When referring to the \text{transformed feature space} it is now one in the same with this higher dimensional space.  However simple linear separations in this transformed feature space transform into non-linear separations when you collapse back into the original space.

If this feature space data is mapped to a Hilbert space referred to with $\Phi$, which allows for traditional Euclidean space vector calculations to be extended to an infinite amount of dimensions using dot products using equations in the form of: $\Phi\left(x_i \right)\cdot\Phi\left(x_j \right)$.  This means we can use what is called a kernel function to avoid ever having to determine the mapping to $\Phi$ and calculate the necessary results directly in the feature space instead.  A kernel function is in the form of: $K(x_{i},x_{j}) = \Phi(x_i) \cdot \Phi(x_j)$ \cite{supervisedMachineLearning}.  There are many commonly used kernels, three of which will be used in this research: linear, polynomial (with degree 3), and radial basis function (Table \ref{tab:kernels}).

\begin{table}[h]
	\centering
	\begin{tabular}{|p{1.5in}|p{4.5in}|}
	\hline
		\textbf{Kernel Function} & \textbf{Mathematical Formula}\\
	\hhline{|=|=|}
		Linear  & $K(x_{i},x_{j}) = \langle x_{i},x_{j}\rangle$ \\
	\hline
		Polynomial & $K(x_{i},x_{j}) = (\langle x_{i},x_{j}\rangle + 1)^d, d: degree$ \\
	\hline
		Radial Basis Function (RBF) & $K(x_{i},x_{j}) = \exp\left(\frac{- \parallel x_i - x_j \parallel^2}{2\sigma^2} \right), \sigma : width\ of\ RBF\ function$ \\
	\hline
	\end{tabular}
	\caption{Kernels that will be used and their mathematical function \cite{intrusionDetectionCostBased}}
	\label{tab:kernels}
\end{table}

Once the training of the SVM is complete using a kernel method of choice the only step left is to pass in all of the testing data and see which side of the hyperplane(s) it falls in order to classify it.  An SVM can at times get very computational intensive and can run very slow, this is often due to the choice of the kernel function as well as the parameters that influence the kernel.  For example, a linear kernel is very simple where as an RBF kernel is much more complex.  Two parameters that are worth mentioning for the SVM training process are $\gamma$ (gamma) and C.

Gamma’s job in the polynomial and RBF kernels is to define how much influence each training vector has on the separating hyperplane(s).  A lower gamma value corresponds to a far influence, when gamma is too small the influence of any support vector may extend to the entire set and the end result would instead just be regions of high density being isolated from other high density areas.  On the converse if gamma is too high then the influence would only extend to the support vector itself.  The C parameter is the penalty cost associated with misclassification, if C is low then classification is more relaxed compared to a higher C which will encourage more support vectors to be chosen and achieve a more accurate division \cite{rbfSVMParameters}.  There is no way to know what are the best values to choose for gamma and C because it depends on the dataset in question and so this must be done by doing testing.

\subsection{Current Support Vector Machine Solutions} \label{sec:SVMSolutions}

Support vector machines are applicable to web threat detection as well but only in the lower layers of the OSI model dealing with network related attacks such as denial of service so far.  One such study used a cost based support vector machine to detect web attacks and was able to detect them with an overall accuracy of 99\% \cite{intrusionDetectionCostBased}.  Likewise, another study compared the usage of an SVM versus an artificial neural network and found the SVM was much faster in comparison along with achieving a 99\% accuracy as well \cite{intrusionDetectionNeural}.  The results of these two studies show that the SVM approach is a viable one and is already useful for practical applications of detecting web threats, so it will be interesting to see how well the algorithm performs for application layer attacks as well as how it compares to the genetic algorithm approach.
