\chapter{Conclusion}

\section{Conclusions}

To reiterate the purpose of this research, current methods of detecting web threats by and large make use of static solutions especially for application layer web threats.  These application layer web threats include such attacks as: SQL injections, cross site scripting, and remote file inclusions which can all be incredibly dangerous to an organization if gone unchecked and account for some of the most notable data breaches in recent years.  Therefore it would be ideal to have a dynamic solution that can improve with the rapidly changing security climate of these web threats, using techniques proposed that employed genetic algorithms served as a starting point for the research.  

The genetic algorithm approach claimed to greatly surpass the traditional signature based systems and similar results were achieved to verify these claims, however issues with the approach began to surface as well.  With the high amount of misclassifications and the high variability of the results of the genetic algorithm was beginning to seem less and less attractive.  The advantage of generating new signatures through the use of the random selections, crossovers, and mutations is ultimately what the genetic algorithm brings to the table.  However when tested to see if the performance could be replicated by singling out these advantages from and simply generating combinations of bitstrings, the results were surprisingly very similar and improved.  If very similar results could be achieved using a fraction of the computational cost of the genetic algorithm, and by some metrics even out performing then the future for using genetic algorithms for this purpose and in this way doesn't seem to be bright.

On the contrary, the support-vector machine approach seemed to be a very good potential candidate as it is designed for this particular type of problem, a classification problem.  It was expected from the beginning that the SVM would be able to out perform the genetic algorithm, and through the results and analysis it was demonstrated that the SVM was able to better optimize for avoiding misclassifications while still maintaing a high success rate.  Although, the stellar performance of the SVM brought into question the reliance on the results of the parser for both algorithms, not just the SVM, which in some ways defeats the original purpose of avoiding the use of signatures and pattern matching in the first place.  However, the features that the parser must identify are much more general and simple than a complex attack itself so this reliance may not be that much of an issue.

In conclusion, through the use of machine learning techniques it is indeed possible to detect application layer web threats including SQL injections, XSS attacks and RFI attacks.  By utilizing the classification power of a support-vector machine it was shown to be possible to achieve upwards of 90-100\% successful detection with a very minimal amount of misclassification, out performing the genetic algorithm approach from previous research.  This is due to how a SVM truly learns with each training input to refine its classification of the parsed results, rather than using machine learning as a mechanism only to produce random permutations that allows the SVM to come out on top.

\section{Future Work}

In regards to future work, it would be interesting to explore how these approaches could be improved with the use of more advanced parsing techniques such as abstract syntax trees or program dependency graphs.  This may allow for a deeper understanding of the code within the requests, perhaps even allowing additional attack types to be detected beyond SQLi, XSS, or RFI.  Additionally, being as the SVM approach seems to be the best of the options examined, additional research looking into how to further optimize its speed and perhaps utilize it as a feedback mechanism in a more fully-fledged detection and prevention system to see how it would be implemented in a real world setting, would be an interesting topic for future research.
