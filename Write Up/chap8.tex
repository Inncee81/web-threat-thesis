\chapter{Conclusion}

\section{Conclusions}

To reiterate the purpose of this research, current methods of detecting web threats by and large make use of static solutions especially for application layer web threats.  These application layer web threats include such attacks as SQL injections, cross site scripting, and remote file inclusions which can be incredibly dangerous to an organization if gone unchecked and account for some of the most notable data breaches.  Therefore it would be ideal for a dynamic solution that can improve to the rapidly changing security climate of these web threats, with techniques proposed that made use of genetic algorithms this served as a starting point for the research.  

The genetic algorithm approach claimed to greatly surpass the traditional signature based systems and similar results were achieved to verify these claims but issues with the algorithm surfaced as well.  With the high amount of mis-classifications and the high variability of the results the genetic algorithm was beginning to seem less and less attractive.  The advantage to generate new signatures through the use of the random selections, crossovers, and mutations is what the genetic algorithm brings to the table.  However when tested to see if this could be replicated by singling out this advantage from the genetic algorithm and simply generating combinations of bitstrings a single time, the results were surprisingly good and very similar.  If very similar results could be achieved in a fraction of the computational cost of the genetic algorithm, and in some ways even better results then the future for using genetic algorithms for this purpose and in this method doesn't seem to be bright.

On the contrary, the support-vector machine approach seemed to be a very good potential candidate as it is designed for this particular type of problem, a classification problem.  It was expected from the beginning that the SVM would be able to out perform the genetic algorithm, and through the results and analysis it was clear that the SVM was able to better optimize for avoiding mis-classifications while still having a high success rate.  Although, the stellar performance of the SVM brought into the question the reliance on the results of the parser for both algorithms, not just the SVM which in some ways defeats the original purpose of avoiding the reliance on signatures and pattern matching.  However, the features that the parser must identify are much more general and simple than a complex attack itself so the reliance may not be that much of an issue.

In conclusion, through the use of machine learning techniques it is indeed possible to detect application layer web threats including SQL injections, XSS attacks and RFI attacks.  By utilizing the classification power of a support-vector machine it has shown to be possible to get upwards of 90-100\% successful detection with a very minimal amount of mis-classification, out performing the genetic algorithm approach of previous research.  It is due to how the SVM truly learns with each training input to refine its classification of the parsed results, rather than using machine learning as a mechanism only to produce random permutations of signatures that allows the SVM to come out on top.

\section{Future Work}

In regards to future work, it would be interesting to explore how these approaches could be improved with the use of more advanced parsing techniques such as abstract syntax trees or program dependency graphs as they would allow for a deeper understanding of the code in the requests, perhaps allowing additional attack types to be detected.  Additionally, being as the SVM approach seems to be the best of the options examined, additional research looking into how to optimize its speed and perhaps utilize it as a feedback mechanism in a more fully-fledged detection and prevention system to see how it would be implemented in a real world setting.
